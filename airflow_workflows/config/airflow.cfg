[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3 or Google Cloud Storage.
# Users must supply a remote location URL (starting with either 's3://'
# or 'gs://') and an Airflow connection id that provides access to the storage location.
remote_log_conn_id =

# Logging class to use
logging_class = airflow.utils.log.file_task_handler.FileTaskHandler

# Log level
log_level = INFO

# Log format
log_format = %(asctime)s - %(name)s - %(levelname)s - %(message)s

# Log filename template
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor
executor = CeleryExecutor

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# The connection string to the Celery backend (Redis)
broker_url = redis://:@redis:6379/0

# Secret key
fernet_key = 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=

# Whether to load the examples that ship with Airflow. It's good to
# have them, but you can turn them off once you are comfortable with
# Airflow's concepts
load_examples = False

# If you do not want to include all the default Airflow DAGs,
# set this to False
load_default_examples = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# Whether to email on retries
email_on_retry = False

# Whether to email on failure
email_on_failure = True

# If email_on_failure is True, the list of email addresses to send alerts to
email = airflow@example.com

# If email_on_failure is True, who should the email be sent from
smtp_from_email = airflow@example.com

# If email_on_failure is True, the smtp server's email user
smtp_user = airflow

# If email_on_failure is True, the smtp server's password
smtp_password = airflow

# If email_on_failure is True, the smtp server's domain
smtp_host = localhost

# If email_on_failure is True, the smtp server's smtp port
smtp_port = 25

# Whether to send emails using the airflow.utils.email.send_email_smtp function
smtp_ssl = False

# How long before timing out a python file import while filling the DagBag
dagbag_import_timeout = 30

# The maximum number of parallel task instances that should run
# per DAG
parallelism = 32

# The maximum number of task instances allowed to run concurrently by
# the scheduler
dag_concurrency = 16

# The number of seconds to wait between polling for new jobs in the
# scheduler
scheduler_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look for new
# tasks and new dag runs) and this defines how often the scheduler
# will look for more tasks
scheduler_dag_file_processor_timeout = 120

# The scheduler will run dag_file_processor_loop every this many seconds
scheduler_job_heartbeat_sec = 1

# When not using pools, tasks are run in the "default pool",
# whose size is guided by this configuration
default_pool_slot_count = 128

# Whether to verify the Celery broker certificates
celery_ssl_active = False

# What SSL version to use when connecting to the Celery broker
celery_ssl_version = ''

# The action to take when a task fails. Options are: 'fail', 'ignore', 'retry'
default_task_retries = 3

[connections]
spark-conn = spark://spark-master:7077